{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MiniHomework#1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDCRGFK1yEHe",
        "colab_type": "text"
      },
      "source": [
        "## 机器学习中的优化  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOcZqPLjyRG5",
        "colab_type": "text"
      },
      "source": [
        "### 问题1\n",
        "假设我们有训练数据$D=\\{(\\mathbf{x}_1,y_1),...,(\\mathbf{x}_n,y_n)\\}$, 其中$(\\mathbf{x}_i,y_i)$为每一个样本，而且$\\mathbf{x}_i$是样本的特征并且$\\mathbf{x}_i\\in \\mathcal{R}^D$, $y_i$代表样本数据的标签（label）, 取值为$0$或者$1$. 在逻辑回归中，模型的参数为$(\\mathbf{w},b)$。对于向量，我们一般用粗体来表达。 为了后续推导的方便，可以把b融入到参数w中。 这是参数$w$就变成 $w=(w_0, w_1, .., w_D)$，也就是前面多出了一个项$w_0$, 可以看作是b，这时候每一个$x_i$也需要稍作改变可以写成 $x_i = [1, x_i]$，前面加了一个1。稍做思考应该能看出为什么可以这么写。\n",
        "\n",
        "请回答以下问题。请用Markdown自带的Latex来编写。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6H2i15uDyTXa",
        "colab_type": "text"
      },
      "source": [
        "####  (a) ```编写逻辑回归的目标函数```\n",
        "请写出目标函数（objective function）, 也就是我们需要\"最小化\"的目标（也称之为损失函数或者loss function)，不需要考虑正则。 把目标函数表示成最小化的形态，另外把$\\prod_{}^{}$转换成$\\log \\sum_{}^{}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu61k20EyWGR",
        "colab_type": "text"
      },
      "source": [
        "逻辑回归的模型为：\n",
        "$$\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "P(Y=1|\\boldsymbol{x};\\boldsymbol{\\omega}) &=\\frac{1}{1+e^{-\\boldsymbol{\\omega} ^T \\boldsymbol{x}}} \\\\\n",
        "P(Y=0|\\boldsymbol{x};\\boldsymbol{\\omega}) &= 1-P(Y=1|\\boldsymbol{x};\\boldsymbol{\\omega})=\\frac{e^{-\\boldsymbol{\\omega} ^T \\boldsymbol{x}}}{1+e^{-\\boldsymbol{\\omega} ^T \\boldsymbol{x}}}\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "$$\n",
        "令$P(Y=1|\\boldsymbol{x};\\boldsymbol{\\omega})=p$, 则$P(Y=0|\\boldsymbol{x};\\boldsymbol{\\omega})=1-p$\n",
        "似然函数为\n",
        "$$\n",
        "\\prod _{i=1}^{n}p^{y_i}(1-p)^{1-y_i}\n",
        "$$\n",
        "对数似然函数为\n",
        "$$\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\\hat{L}(\\omega) &= \\sum_{i=1}^n [y_i \\log p + (1-y_i) \\log (1-p)]\\\\\n",
        "&= \\sum_{i=1}^n [y_i \\log{\\frac{p}{1-p}} + \\log{(1-p)}]\\\\\n",
        "&= \\sum_{i=1}^n [y_i \\boldsymbol{\\omega} ^T \\boldsymbol{x_i} - \\log{(1+e^{\\boldsymbol{\\omega} ^T \\boldsymbol{x_i}})}]\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "$$\n",
        "最大化对数似然函数等价于最小化\n",
        "$$\n",
        "L(\\omega) = \\sum_{i=1}^n [-y_i\\boldsymbol{\\omega} ^T \\boldsymbol{x_i} + \\log{(1+e^{\\boldsymbol{\\omega} ^T \\boldsymbol{x_i}})}]\n",
        "$$\n",
        "\n",
        "\n",
        "**以下转化为向量形式：**\n",
        "\n",
        "定义矩阵$\\mathbf{X}=(\\boldsymbol{x_1} \\  \\boldsymbol{x_2} \\ \\cdots \\  \\boldsymbol{x_n})^T = \\begin{pmatrix}\n",
        "\\boldsymbol{x_1}^T\\\\ \n",
        "\\boldsymbol{x_2}^T\\\\ \n",
        "\\vdots \\\\ \n",
        "\\boldsymbol{x_n}^T\n",
        "\\end{pmatrix}$，$\\mathbf{Y}=\\begin{pmatrix}\n",
        "y_1\\\\ \n",
        "y_n\\\\ \n",
        "\\vdots \\\\ \n",
        "y_n\n",
        "\\end{pmatrix}$\n",
        "\n",
        "则，目标函数表达为向量形式如下：\n",
        "$$\n",
        "\\mathcal{L}(\\boldsymbol{\\omega})=- \\mathbf{Y}^T \\mathbf{X} \\boldsymbol{\\omega} + \\mathbf{1}^T \\log{(\\mathbf{1}+\\exp{(\\mathbf{X} \\boldsymbol{\\omega})})}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1gOQ43_yZAE",
        "colab_type": "text"
      },
      "source": [
        "####  (b) ```求解对w的一阶导数```\n",
        "为了做梯度下降法，我们需要对参数$w$求导，请把$L(w)$对$w$的梯度计算一下："
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czXXuXyrybQb",
        "colab_type": "text"
      },
      "source": [
        "记$\\sigma(x)$：\n",
        "$$\n",
        "\\sigma(x)=\\frac{1}{1+\\exp (-x)}\n",
        "$$\n",
        "\n",
        "\n",
        "则$L(\\boldsymbol{\\omega})$对$\\boldsymbol{\\omega}$的梯度为：\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\frac{\\partial L(\\boldsymbol{w})}{\\partial \\boldsymbol{w}} &= \\sum_{i=1}^n (-\\boldsymbol{x_i} y_i + \\frac{\\boldsymbol{x_i} e^{\\boldsymbol{\\omega} ^T \\boldsymbol{x_i}}}{1+ e^{\\boldsymbol{\\omega} ^T \\boldsymbol{x_i}}})\\\\ \n",
        "&= \\sum^n_{i=1} \\boldsymbol{x_i}(\\sigma(\\boldsymbol{\\omega} ^T \\boldsymbol{x_i}) - y_i)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "\n",
        "则把$L(\\boldsymbol{\\omega})$对$\\boldsymbol{\\omega}$的梯度表示为向量形式如下：\n",
        "$$\n",
        "\\frac{\\partial L(\\boldsymbol{w})}{\\partial \\boldsymbol{w}} = \\mathbf{X}^T(\\sigma(\\mathbf{X} \\boldsymbol{\\omega}) - \\mathbf{Y})\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtXz4DJayd7F",
        "colab_type": "text"
      },
      "source": [
        "####  (c) ```求解对w的二阶导数```\n",
        "在上面结果的基础上对$w$求解二阶导数，也就是再求一次导数。 这个过程需要回忆一下线性代数的部分 ^^。 参考： matrix cookbook: https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf, 还有 Hessian Matrix。 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0vN682dygYO",
        "colab_type": "text"
      },
      "source": [
        "sigmoid函数的导数为：\n",
        "$$\n",
        "\\sigma^{\\prime}(x)=\\sigma(x) (1-\\sigma(x))=\\frac{\\exp (-x)}{(1+\\exp (-x))^2}\n",
        "$$\n",
        "\n",
        "目标函数对 $\\boldsymbol{\\omega}$ 的二阶导数为：\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\frac{\\partial^2 L(\\boldsymbol{w})}{\\partial^2 \\boldsymbol{w}} &= \\sum_{i=1}^n \\boldsymbol{x_i} \\sigma^{\\prime}(\\boldsymbol{\\omega}^T \\boldsymbol{x_i}) \\\\ \n",
        "&= \\sum_{i=1}^n \\boldsymbol{x_i} \\boldsymbol{x_i}^T \\sigma(\\boldsymbol{\\omega}^T \\boldsymbol{x_i}) (1-\\sigma(\\boldsymbol{\\omega}^T \\boldsymbol{x_i}))\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "\n",
        "表示为向量形式为：\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\frac{\\partial^2 L(\\boldsymbol{w})}{\\partial \\boldsymbol{w} \\partial \\boldsymbol{w}^T}&=\\mathbf{X}^T \\sigma^{\\prime}(\\mathbf{X} \\boldsymbol{\\omega}) \\mathbf{X} \\\\\n",
        "&=\\mathbf{X}^T \\operatorname{diag}(\\sigma(\\mathbf{X} \\boldsymbol{\\omega}) \\odot (1-\\sigma(\\mathbf{X} \\boldsymbol{\\omega}))) \\mathbf{X}\n",
        "\\end{aligned}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13zHGXIjyioD",
        "colab_type": "text"
      },
      "source": [
        "#### (d) ```证明逻辑回归目标函数是凸函数```\n",
        "试着证明逻辑回归函数是凸函数。假设一个函数是凸函数，我们则可以得出局部最优解即为全局最优解，所以假设我们通过随机梯度下降法等手段找到最优解时我们就可以确认这个解就是全局最优解。证明凸函数的方法有很多种，在这里我们介绍一种方法，就是基于二次求导大于等于0。比如给定一个函数$f(x)=x^2-3x+3$，做两次\n",
        "求导之后即可以得出$f''(x)=2 > 0$，所以这个函数就是凸函数。类似的，这种理论也应用于多元变量中的函数上。在多元函数上，只要证明二阶导数是posititive semidefinite即可以。 问题（c）的结果是一个矩阵。 为了证明这个矩阵（假设为H)为Positive Semidefinite，需要证明对于任意一个非零向量$v\\in \\mathcal{R}$, 需要得出$v^{T}Hv >=0$\n",
        "请写出详细的推导过程："
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDqtIVYoylIS",
        "colab_type": "text"
      },
      "source": [
        "记 $\\mathbf{H}=\\mathbf{X}^T \\operatorname{diag}(\\sigma(\\mathbf{X} \\boldsymbol{\\omega}) \\odot (1-\\sigma(\\mathbf{X} \\boldsymbol{\\omega}))) \\mathbf{X}$ ，则有 $\\mathbf{H}^T=\\mathbf{H}$ 即 $\\mathbf{H}$ 为对称矩阵。若$v_i \\geq 0, \\ i=1, 2, \\ldots, p$，则向量$\\boldsymbol{v}=(v_1, v_2, \\cdots, v_p)^T \\neq \\boldsymbol{0}$。\n",
        "\n",
        "证明：\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\boldsymbol{v}^T \\mathbf{H} \\boldsymbol{v} &= \\boldsymbol{v}^T \\mathbf{X}^T \\operatorname{diag}(\\sigma^{\\prime}(\\mathbf{X} \\boldsymbol{\\omega})) \\mathbf{X} \\boldsymbol{v} \\\\\n",
        "&=(\\mathbf{X} \\boldsymbol{v})^T \\operatorname{diag}(\\sigma^{\\prime}(\\mathbf{X} \\boldsymbol{\\omega})) \\mathbf{X} \\boldsymbol{v} \\\\\n",
        "& = \\sum_{i=1}^n \\boldsymbol{x_i} v_i (\\boldsymbol{x_i} v_i)^T \\sigma{(\\boldsymbol{\\omega}^T \\boldsymbol{x_i})} (1-\\sigma{(\\boldsymbol{\\omega}^T \\boldsymbol{x_i})}) \\\\\n",
        "& = \\sum_{i=1}^n v_i^2 {\\left \\| \\boldsymbol{x_i} \\right \\|}^2  \\sigma{(\\boldsymbol{\\omega}^T \\boldsymbol{x_i})} (1-\\sigma{(\\boldsymbol{\\omega}^T \\boldsymbol{x_i})}) \\\\\n",
        "& \\geq 0\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "\n",
        "其中，$v_i$， $\\sigma{(\\boldsymbol{\\omega}^T \\boldsymbol{x_i})}$ 均为实数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2gAxXvDx7VG",
        "colab_type": "text"
      },
      "source": [
        "### 问题2\n",
        "证明p-norm是凸函数， p-norm的定义为：\n",
        "$||x||_p=(\\sum_{i=1}^{n}|x_i|^p)^{1/p}$\n",
        "\n",
        "hint: Minkowski’s Inequality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uf7Q0WyyAr0",
        "colab_type": "text"
      },
      "source": [
        "定义函数$f$ 是向量空间$V$到实数空间$\\mathbb{R}$的映射，即\n",
        "$$\n",
        "f:V\\rightarrow\\mathbb{R}\n",
        "$$\n",
        "则函数$f$ 是凸函数，当满足：\n",
        "$$\n",
        "\\forall v, w \\in V, \\lambda \\in[0,1]: f(\\lambda v+(1-\\lambda) w) \\leq \\lambda f(v)+(1-\\lambda) f(w)\n",
        "$$\n",
        "当映射$f$ 为$p$-norm时，由**闵可夫斯基不等式**（Minkowski inequality）得\n",
        "$$\n",
        "\\|\\lambda v+(1-\\lambda) w\\|_p \\leq\\|\\lambda v\\|_p+\\|(1-\\lambda) w\\|_p=\\lambda\\|v\\|_p+(1-\\lambda)\\|w\\|_p\n",
        "$$\n",
        "即 $p-norm$ 是凸函数。\n",
        "\n",
        "其中，\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\|\\lambda x\\|_p &= (\\sum_{i=1}^n |\\lambda x_i|^p)^{\\frac{1}{p}}\\\\\n",
        "&=(|\\lambda|^p \\sum_{i=1}^n |x_i|^p)^{\\frac{1}{p}} \\\\\n",
        "&=\\lambda (\\sum_{i=1}^n |x_i|^p)^{\\frac{1}{p}} \\\\\n",
        "&=\\lambda \\| x \\|_p\n",
        "\\end{aligned}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hA1jDrPDxO2m",
        "colab_type": "text"
      },
      "source": [
        "### 问题3\n",
        "在课堂里我们讲过Transportation problem. 重新描述问题： 有两个城市北京和上海，分别拥有300件衣服和500件衣服，另外有三个城市分别是1，2，3分别需要200，300，250件衣服。现在需要把衣服从北京和上海运送到城市1，2，3。 我们假定每运输一件衣服会产生一些代价，比如：\n",
        "- 北京 -> 1:  5\n",
        "- 北京 -> 2:  6\n",
        "- 北京 -> 3:  4\n",
        "- 上海 -> 1:  6\n",
        "- 上海 -> 2:  3\n",
        "- 上海 -> 3:  7\n",
        "\n",
        "最后的值是单位cost. \n",
        "\n",
        "问题：我们希望最小化成本。 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DRHXOTHxU0G",
        "colab_type": "text"
      },
      "source": [
        "```(a)``` 请写出linear programming formulation。 利用标准的写法(Stanford form)，建议使用矩阵、向量的表示法。 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3ATK9m9xVsB",
        "colab_type": "text"
      },
      "source": [
        "$x_1,x_2,x_3$分别表示北京运往城市1,2,3的衣服数，$x_4,x_5,x_6$分别表示上海运往城市1,2,3的衣服数，则最优化问题描述如下：\n",
        "\n",
        "$$\\min {5x_1 + 6 x_2 + 4 x_3 + 6 x_4 + 3 x_5 + 7x_6}$$\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "s.t.\\ &x_1 + x_2 + x_3 \\leqslant 300\\\\\n",
        "      &x_4 +x_5 + x_6 \\leqslant 500\\\\\n",
        "      &x_1 + x_4 = 200\\\\     \n",
        "      &x_2+x_5 = 300\\\\\n",
        "       &x_3+x_6 = 250\\\\\n",
        "      &x_1,x_2,x_3,x_4,x_5,x_6\\geqslant 0\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "转换成矩阵表示为：\n",
        "$$\n",
        "\\begin{align}\n",
        "&c^Tx\\\\\n",
        "\\\\\n",
        "s.t. \\ &A_{ub}x \\leqslant b_{ub}\\\\\n",
        "&A_{eq}=b_{eq}\\\\\n",
        "&x \\geqslant 0\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "其中，$x=(x_1,x_2,x_3,x_4,x_5,x_6)^T$， $c= (5,6,4,6,3,7)^T$，\n",
        "$A_{ub}=\\begin{bmatrix}\n",
        "1 & 1 &  1& 0 & 0 &0 \\\\ \n",
        "0 & 0 &  0& 1 & 1 &1 \\\\ \n",
        "\\end{bmatrix}$，\n",
        "$A_{eq} = \\begin{bmatrix}\n",
        "1 & 0 &  0& 1 & 0 &0 \\\\ \n",
        "0 & 1 &  0&  0& 1 &0 \\\\ \n",
        "0 & 0 &  1&  0& 0 &1 \\\\\n",
        "\\end{bmatrix}\n",
        "$\n",
        "$b_{ub}=(300,500)^T$,\n",
        "$b_{eq}=(200,300,250)^T$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaWfdPfxx4Fw",
        "colab_type": "text"
      },
      "source": [
        "```(b)``` 利用lp solver求解最优解。 参考：\n",
        "https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.linprog.html\n",
        "    或者： http://cvxopt.org/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xI-640_wZu-",
        "colab_type": "code",
        "outputId": "36091d17-07ee-4ae7-cbe4-b1067b2779d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# your implementation\n",
        "from scipy import  optimize as opt\n",
        "import numpy as np\n",
        "\n",
        "c=np.array([5,6,4,6,3,7])\n",
        "\n",
        "a_ub=np.array([[1,1,1,0,0,0],[0,0,0,1,1,1]])\n",
        "b_ub=np.array([300, 500])\n",
        "\n",
        "a_eq = np.array([[1,0,0,1,0,0],[0,1,0,0,1,0],[0,0,1,0,0,1]])\n",
        "b_eq = np.array([200, 300, 250])\n",
        "\n",
        "ans=opt.linprog(c,a_ub,b_ub, a_eq, b_eq, method='revised simplex')\n",
        "print(ans)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     con: array([0., 0., 0.])\n",
            "     fun: 3050.0\n",
            " message: 'Optimization terminated successfully.'\n",
            "     nit: 6\n",
            "   slack: array([ 0., 50.])\n",
            "  status: 0\n",
            " success: True\n",
            "       x: array([ 50.,   0., 250., 150., 300.,   0.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqcIeFjGxlhg",
        "colab_type": "text"
      },
      "source": [
        "```(c)```: 试着把上述LP转化成Dual formulation，请写出dual form. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq6A1ZYTNliY",
        "colab_type": "text"
      },
      "source": [
        "[对偶线性规划](http://netedu.xauat.edu.cn/jpkc/netedu/jpkc/ycx/kcjy/kejian/pdf/02.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjQ9PQTS4Blr",
        "colab_type": "text"
      },
      "source": [
        "$c=(5,6,4,6,3,7)^T$\n",
        "\n",
        "\n",
        "$A=\\begin{bmatrix}\n",
        "1 & 1 &  1& 0 & 0 &0 \\\\ \n",
        "0 & 0 &  0& 1 & 1 &1 \\\\\n",
        "1 & 0 &  0& 1 & 0 &0 \\\\ \n",
        "0 & 1 &  0&  0& 1 &0 \\\\ \n",
        "0 & 0 &  1&  0& 0 &1 \\\\\n",
        "-1 & 0 &  0& -1 & 0 &0 \\\\ \n",
        "0 & -1 &  0&  0& -1 &0 \\\\ \n",
        "0 & 0 &  -1&  0& 0 &-1 \\\\\n",
        "\\end{bmatrix}$\n",
        "\n",
        "$b=(300, 500, 200, 300, 250, -200, -300, -250)^T$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S03-AM2wxpxE",
        "colab_type": "text"
      },
      "source": [
        "$b^Ty$\n",
        "\n",
        "$A^Ty \\geqslant c$\n",
        "\n",
        "$y \\geqslant 0$\n",
        "\n",
        "$\\max \\ 300y_1 + 500 y_2 + 200 y_3 + 300 y_4+ 250 y_5 - 200 y_6 - 300 y_7 -250 y_8$\n",
        "\n",
        "$y_1+y_3 - y_6 \\geqslant 5\\\\\n",
        "y_1 + y_4 - y_7 \\geqslant 6\\\\\n",
        "y_1 + y_5 - y_8 \\geqslant 4\\\\\n",
        "y_2 + y_3 - y_6 \\geqslant 6\\\\\n",
        "y_2 + y_4 - y_7 \\geqslant 3\\\\\n",
        "y_2 + y_5 - y_8 \\geqslant 7$\n",
        "\n",
        "$y_i \\geqslant 0, i=1,...,8$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6J5oB556RCC",
        "colab_type": "code",
        "outputId": "cd71a6e6-fadf-40a0-e548-b7fd97a83af6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "c=np.array([300, 500, 200, 300, 250, -200, -300, -250])\n",
        "\n",
        "a=np.array([[-1,0,-1,0,0,1,0,0], [-1,0,0,-1,0,0,1,0], [-1,0,0,0,-1,0,0,1], [0,-1,-1,0,0,1,0,0], [0,-1,0,-1,0,0,1,0], [0,-1,0,0,-1,0,0,1]])\n",
        "\n",
        "b=np.array([-5, -6, -4, -6, -3, -7])\n",
        "\n",
        "ans=opt.linprog(c, a, b, method='revised simplex')\n",
        "print(ans)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     con: array([], dtype=float64)\n",
            "     fun: 4750.0\n",
            " message: 'Optimization terminated successfully.'\n",
            "     nit: 10\n",
            "   slack: array([1., 0., 3., 0., 3., 0.])\n",
            "  status: 0\n",
            " success: True\n",
            "       x: array([0., 0., 6., 6., 7., 0., 0., 0.])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}